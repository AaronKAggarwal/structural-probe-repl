# configs/training/adam_modern_scheduler.yaml
# A modern training regimen using the Adam optimizer and a standard ReduceLROnPlateau scheduler.
# This config is self-contained and does NOT inherit from others.

optimizer:
  name: Adam
  lr: 0.001

loss_function: "l1_squared_diff"
clip_grad_norm: null
batch_size: 20 # Using the H&M batch size for consistency
epochs: 40
patience: 10
early_stopping_metric: "loss"     # Monitor dev loss
early_stopping_delta: 0.0001
save_every_epoch_checkpoint: false
save_checkpoint_every_n_epochs: -1
eval_on_train_epoch_end: true
limit_eval_batches: -1
limit_train_batches: -1

# Disable the H&M custom scheduler
lr_scheduler_with_reset:
  enable: false

# Enable and configure the standard PyTorch scheduler
scheduler:
  enable: true
  name: "ReduceLROnPlateau"
  mode: "min"           # 'min' because we are monitoring loss
  factor: 0.1
  patience: 5