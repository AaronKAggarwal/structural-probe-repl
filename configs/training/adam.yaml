optimizer:
  name: Adam
  lr: 0.001
loss_function: "l1_squared_diff"
clip_grad_norm: null
batch_size: 64
epochs: 40
patience: 10
early_stopping_metric: "loss"
early_stopping_delta: 0.0
save_every_epoch_checkpoint: false
save_checkpoint_every_n_epochs: -1
eval_on_train_epoch_end: true
limit_eval_batches: -1
limit_train_batches: -1