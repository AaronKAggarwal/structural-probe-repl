optimizer:
  name: Adam
  lr: 0.001
loss_function: "l1_squared_diff"
clip_grad_norm: null
batch_size: 20
epochs: 40
patience: 10
early_stopping_metric: "loss"
early_stopping_delta: 0.0
save_every_epoch_checkpoint: false
save_checkpoint_every_n_epochs: -1
eval_on_train_epoch_end: true
limit_eval_batches: -1
limit_train_batches: -1
eval_every_n_epochs: 1

scheduler:
  enable: false
  name: "ReduceLROnPlateau"
  mode: "min"
  factor: 0.1
  patience: 10

lr_scheduler_with_reset:
  enable: false
  lr_decay_factor: 0.5
  lr_decay_patience: 5
  min_lr: 1.0e-5