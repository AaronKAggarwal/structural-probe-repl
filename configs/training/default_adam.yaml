# configs/training/default_adam.yaml

optimizer:
  name: "Adam"
  lr: 0.001                 # Initial learning rate
  weight_decay: 0.0
  betas: [0.9, 0.999]       # Adam default betas
  eps: 1.0e-8               # Adam default epsilon

batch_size: 32              # Default batch size, can be overridden by experiment
epochs: 50                  # Default max epochs

# Early stopping for the entire training run
patience: 10                # Stop if no improvement in dev metric for this many epochs
early_stopping_metric: "loss" # Default metric to monitor (options: loss, uuas, spearmanr, root_acc)
                            # Mode ("min" or "max") for this metric will be inferred in train_probe.py
early_stopping_delta: 0.0001 # Min change to be considered an improvement for early stopping

# H&M-style LR decay with optimizer reset
# This is separate from the main early stopping above.
# It reduces LR and resets optimizer state if dev metric (same one as above)
# doesn't improve for lr_decay_patience epochs.
lr_scheduler_with_reset:
  enable: true              # Set to true to enable this custom LR decay schedule
  lr_decay_factor: 0.1      # Factor to multiply LR by (e.g., 0.1 for 10x reduction)
  lr_decay_patience: 3      # Number of epochs without dev metric improvement before decaying LR
  min_lr: 1.0e-6            # Minimum learning rate to decay to
  # monitor_metric and monitor_mode for LR decay will default to early_stopping_metric and its mode
  # delta for LR decay improvement check will default to early_stopping_delta

clip_grad_norm: null        # e.g., 1.0 to enable, or null to disable