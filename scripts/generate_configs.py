# scripts/generate_configs.py
import argparse
from pathlib import Path

def _normalize_lang_key(s: str) -> str:
    return s.strip().lower().replace(" ", "_")


def _discover_from_metadata_csv() -> dict:
    meta_csv = Path("data/lang_stats/ud_metadata.csv")
    if not meta_csv.exists():
        return {}
    import csv
    mapping = {}
    with open(meta_csv, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            slug = (row.get("treebank_slug") or "").strip()
            lang = (row.get("language") or "").strip()
            if not slug:
                continue
            if lang:
                mapping[_normalize_lang_key(lang)] = slug
            # Also allow selecting by slug directly
            mapping[slug] = slug
    return mapping


def discover_available_languages():
    """
    Discover available datasets/languages.
    Priority: metadata CSV if present, else scan configs/dataset.
    Returns a dict mapping friendly keys (language or slug) to dataset slugs.
    """
    # Prefer metadata-driven discovery for accuracy
    mapping = _discover_from_metadata_csv()
    if mapping:
        return mapping

    dataset_dir = Path("configs/dataset")
    if not dataset_dir.exists():
        return {}

    languages = {}
    for dataset_path in dataset_dir.iterdir():
        if not dataset_path.is_dir():
            continue
        dataset_name = dataset_path.name
        expected_config = dataset_path / f"{dataset_name}.yaml"
        if not expected_config.exists():
            continue
        # Always allow selecting by slug name
        languages[dataset_name] = dataset_name
        # Try to infer a plain language key from UD slug
        lower = dataset_name.lower()
        if lower.startswith("ud_") and "-" in dataset_name:
            try:
                lang_key = dataset_name.split("_", 1)[1].split("-", 1)[0]
                languages[_normalize_lang_key(lang_key)] = dataset_name
            except Exception:
                pass
        elif lower.startswith("ud_"):
            languages[_normalize_lang_key(lower.replace("ud_", ""))] = dataset_name
        elif "_" in lower:
            languages[_normalize_lang_key(lower.split("_", 1)[0])] = dataset_name
    return languages

def show_available_languages():
    """Display available languages and their corresponding dataset configs."""
    languages = discover_available_languages()
    if not languages:
        print("No language datasets found in configs/dataset/")
        print("Expected structure: configs/dataset/{dataset_name}/{dataset_name}.yaml")
        return

    print("Available languages:")
    # show in sorted order and de-duplicate values for readability
    for lang in sorted(languages.keys()):
        dataset = languages[lang]
        print(f"  {lang}: {dataset}")
    print()

def create_config_files(
    model_hf_name: str,
    model_name_sanitized: str,
    dataset_name: str,
    experiment_group: str,
    model_dimension: int,
    num_layers: int,
    base_output_dir: str = "configs",
):
    """
    Generates a full suite of Hydra config files for probing a model.

    This will create:
    1. An `embeddings` config for each layer.
    2. A `logging` config for each layer and probe type.
    3. An `experiment` config for each layer and probe type.
    """
    print(f"--- Generating configs for model '{model_hf_name}' ---")
    print(f"Dataset: {dataset_name}, Experiment Group: {experiment_group}")
    print(f"Layers: 0 to {num_layers - 1}, Dimension: {model_dimension}")
    print("-" * 50)

    base_path = Path(base_output_dir)

    embeddings_dir = base_path / "embeddings" / dataset_name / model_name_sanitized
    logging_dir = base_path / "logging" / f"{dataset_name}_{model_name_sanitized}"
    # Include dataset_name to avoid collisions when generating for multiple treebanks
    experiment_dir = base_path / "experiment" / experiment_group / dataset_name / model_name_sanitized

    # Create directories if they don't exist
    embeddings_dir.mkdir(parents=True, exist_ok=True)
    logging_dir.mkdir(parents=True, exist_ok=True)
    (experiment_dir / "dist").mkdir(parents=True, exist_ok=True)
    (experiment_dir / "depth").mkdir(parents=True, exist_ok=True)

    # Path template for HDF5 files (matches extract_embeddings.py default output)
    hdf5_path_template = f"data_staging/embeddings/{dataset_name}/{model_name_sanitized}/{dataset_name}_conllu_{{split}}_layers-all_align-mean.hdf5"

    for layer_idx in range(num_layers):
        layer_name = f"L{layer_idx}"

        # --- 1. Create Embeddings Config ---
        emb_path = embeddings_dir / f"{layer_name}.yaml"
        emb_content = f"""
# Auto-generated by generate_configs.py
source_model_name: "{model_hf_name}"
layer_index: {layer_idx}
dimension: {model_dimension}
paths:
  train: "{hdf5_path_template.format(split='train')}"
  dev: "{hdf5_path_template.format(split='dev')}"
  test: "{hdf5_path_template.format(split='test')}"
""".strip()
        with open(emb_path, "w") as f:
            f.write(emb_content)
        print(f"Created: {emb_path}")

        # --- 2. Create Configs for each Probe Type ---
        # Use shortened name "dist" to match existing folder/file conventions
        for probe_type in ["dist", "depth"]:
            # Use the SAME rank for both probes to avoid capacity confounds.
            probe_rank = 128

            # Use full probe name for human-readable fields (tags, probe config reference)
            probe_full_name = "distance" if probe_type == "dist" else "depth"

            # --- 2a. Create Logging Config ---
            log_path = logging_dir / f"{probe_type}_{layer_name}.yaml"
            log_content = f"""
# Auto-generated by generate_configs.py
wandb:
  tags: ["{experiment_group}", "{dataset_name}", "{model_name_sanitized}", "layer${{embeddings.layer_index}}", "{probe_full_name}", "r${{probe.rank}}"]
""".strip()
            with open(log_path, "w") as f:
                f.write(log_content)
            print(f"Created: {log_path}")

            # --- 2b. Create Experiment Config ---
            exp_path = experiment_dir / probe_type / f"{layer_name}.yaml"

            depth_overrides = ""
            if probe_type == "depth":
                depth_overrides = """
evaluation:
  metrics: ["spearmanr_hm", "root_acc"]
""".strip()

            exp_name_suffix = f"{probe_type.upper()}_R{probe_rank}"

            exp_content = f"""
# Auto-generated by generate_configs.py
name: {experiment_group}/{model_name_sanitized}/{probe_type}/{layer_name}

defaults:
  - _self_
  - /dataset: {dataset_name}/{dataset_name}
  - /embeddings: {dataset_name}/{model_name_sanitized}/{layer_name}
  - /probe: {probe_full_name}
  - /probe_rank@probe: "{probe_rank}"
  - /training: adam_modern_scheduler
  - /evaluation: default
  - /runtime: mps
  - /logging: {dataset_name}_{model_name_sanitized}/{probe_type}_{layer_name}

# Per-experiment training cadence (tests and real runs)
training:
  eval_every_n_epochs: 1
  eval_on_train_every_n_epochs: 40

{depth_overrides}

logging:
  experiment_name: "{dataset_name.upper()}_{model_name_sanitized.upper()}_{layer_name}_{exp_name_suffix}"
""".strip()
            with open(exp_path, "w") as f:
                f.write(exp_content)
        print(f"Created: {exp_path}")
        print("-" * 20)

    print(f"\n--- Generation Complete for {num_layers} layers ---")
    # ... (rest of the script is unchanged) ...
    print("\nNext Steps:")
    print("1. Extract the embeddings for the model:")
    print(f"   poetry run python scripts/extract_embeddings.py dataset={dataset_name}/{dataset_name} model={model_name_sanitized} job.layers_to_extract=all")
    print("\n2. Run a sweep over all layers for the distance probe:")
    print(f"   poetry run python scripts/train_probe.py -m experiment={experiment_group}/{model_name_sanitized}/dist/L0,L1,L2,...")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate a full suite of Hydra config files for a probing experiment sweep."
    )


    # Show available languages first
    show_available_languages()

    # Required language parameter
    parser.add_argument("--language", type=str, required=True,
                        help="Language to generate configs for. Use 'list' to see available languages, or 'custom' for manual specification.")

    # Model parameters
    parser.add_argument("--model_hf_name", type=str, default="bert-base-multilingual-cased", help="Hugging Face model identifier.")
    parser.add_argument("--model_name_sanitized", type=str, default="bert-base-multilingual-cased", help="Filesystem-friendly name for the model.")
    parser.add_argument("--model_dimension", type=int, default=768, help="Hidden dimension size of the model embeddings.")
    parser.add_argument("--num_layers", type=int, default=13, help="Total number of layers to generate configs for (embedding layer 0 + N transformer layers).")

    # Optional overrides
    parser.add_argument("--dataset_name", type=str, default=None, help="Override dataset name (for custom language or non-standard naming).")
    parser.add_argument("--experiment_group", type=str, default=None, help="Override experiment group name (for custom organization).")

    args = parser.parse_args()

    # Handle special cases
    if args.language == "list":
        print("Use one of the available languages shown above.")
        exit(0)
    
    # Discover available languages
    available_languages = discover_available_languages()

    if args.language == "custom":
        if not args.dataset_name or not args.experiment_group:
            print("ERROR: When using --language custom, you must specify both --dataset_name and --experiment_group")
            exit(1)
        dataset_name = args.dataset_name
        experiment_group = args.experiment_group
    else:
        if args.language not in available_languages:
            print(f"ERROR: Language '{args.language}' not found.")
            print("Available languages:", list(available_languages.keys()))
            print("Use --language custom with --dataset_name and --experiment_group for manual specification.")
            exit(1)

        # Auto-determine dataset and experiment group
        dataset_name = args.dataset_name or available_languages[args.language]
        experiment_group = args.experiment_group or f"baselines_{args.language}"

    create_config_files(
        model_hf_name=args.model_hf_name,
        model_name_sanitized=args.model_name_sanitized.replace('/', '-'),
        dataset_name=dataset_name,
        experiment_group=experiment_group,
        model_dimension=args.model_dimension,
        num_layers=args.num_layers,
    )