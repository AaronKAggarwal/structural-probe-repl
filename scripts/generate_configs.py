# scripts/generate_configs.py
import argparse
import os
from pathlib import Path

def create_config_files(
    model_hf_name: str,
    model_name_sanitized: str,
    dataset_name: str,
    experiment_group: str,
    model_dimension: int,
    num_layers: int,
    base_output_dir: str = "configs",
):
    """
    Generates a full suite of Hydra config files for probing a model.

    This will create:
    1. An `embeddings` config for each layer.
    2. A `logging` config for each layer and probe type.
    3. An `experiment` config for each layer and probe type.
    """
    print(f"--- Generating configs for model '{model_hf_name}' ---")
    print(f"Dataset: {dataset_name}, Experiment Group: {experiment_group}")
    print(f"Layers: 0 to {num_layers - 1}, Dimension: {model_dimension}")
    print("-" * 50)

    base_path = Path(base_output_dir)

    # --- FIX: Use model_name_sanitized consistently for all directory paths ---
    embeddings_dir = base_path / "embeddings" / dataset_name / model_name_sanitized
    logging_dir = base_path / "logging" / f"{dataset_name}_{model_name_sanitized}"
    experiment_dir = base_path / "experiment" / experiment_group / model_name_sanitized # This line is the fix
    # --- END FIX ---

    # Create directories if they don't exist
    embeddings_dir.mkdir(parents=True, exist_ok=True)
    logging_dir.mkdir(parents=True, exist_ok=True)
    (experiment_dir / "dist").mkdir(parents=True, exist_ok=True)
    (experiment_dir / "depth").mkdir(parents=True, exist_ok=True)
    
    # Path template for HDF5 files (matches extract_embeddings.py default output)
    hdf5_path_template = f"data_staging/embeddings/{dataset_name}/{model_name_sanitized}/{dataset_name}_conllu_{{split}}_layers-all_align-mean.hdf5"

    for layer_idx in range(num_layers):
        layer_name = f"L{layer_idx}"

        # --- 1. Create Embeddings Config ---
        emb_path = embeddings_dir / f"{layer_name}.yaml"
        emb_content = f"""
# Auto-generated by generate_configs.py
source_model_name: "{model_hf_name}"
layer_index: {layer_idx}
dimension: {model_dimension}
paths:
  train: "{hdf5_path_template.format(split='train')}"
  dev: "{hdf5_path_template.format(split='dev')}"
  test: "{hdf5_path_template.format(split='test')}"
""".strip()
        with open(emb_path, "w") as f:
            f.write(emb_content)
        print(f"Created: {emb_path}")

        # --- 2. Create Configs for each Probe Type ---
        # Use shortened name "dist" to match existing folder/file conventions
        for probe_type in ["dist", "depth"]:
            
            probe_rank = model_dimension if probe_type == "depth" else 128

            # Use full probe name for human-readable fields (tags, probe config reference)
            probe_full_name = "distance" if probe_type == "dist" else "depth"
            
            # --- 2a. Create Logging Config ---
            log_path = logging_dir / f"{probe_type}_{layer_name}.yaml"
            log_content = f"""
# Auto-generated by generate_configs.py
wandb:
  tags: ["{experiment_group}", "{dataset_name}", "{model_name_sanitized}", "layer${{embeddings.layer_index}}", "{probe_full_name}", "r${{probe.rank}}"]
""".strip()
            with open(log_path, "w") as f:
                f.write(log_content)
            print(f"Created: {log_path}")

            # --- 2b. Create Experiment Config ---
            exp_path = experiment_dir / probe_type / f"{layer_name}.yaml"
            
            depth_overrides = ""
            if probe_type == "depth":
                depth_overrides = """
evaluation:
  metrics: ["spearmanr_hm", "root_acc"]
""".strip()

            exp_name_suffix = f"{probe_type.upper()}_R{probe_rank}"
            
            exp_content = f"""
# Auto-generated by generate_configs.py
name: {experiment_group}/{model_name_sanitized}/{probe_type}/{layer_name}

defaults:
  - _self_
  - /dataset: {dataset_name}/{dataset_name}
  - /embeddings: {dataset_name}/{model_name_sanitized}/{layer_name}
  - /probe: {probe_full_name}
  - /probe_rank@probe: "{probe_rank}"
  - /training: adam_modern_scheduler
  - /evaluation: default
  - /runtime: mps
  - /logging: {dataset_name}_{model_name_sanitized}/{probe_type}_{layer_name}

{depth_overrides}

logging:
  experiment_name: "{dataset_name.upper()}_{model_name_sanitized.upper()}_{layer_name}_{exp_name_suffix}"
""".strip()
            with open(exp_path, "w") as f:
                f.write(exp_content)
            print(f"Created: {exp_path}")
        print("-" * 20)

    print(f"\n--- Generation Complete for {num_layers} layers ---")
    # ... (rest of the script is unchanged) ...
    print("\nNext Steps:")
    print("1. Extract the embeddings for the model:")
    print(f"   poetry run python scripts/extract_embeddings.py dataset={dataset_name} model={model_name_sanitized} job.layers_to_extract=all")
    print("\n2. Run a sweep over all layers for the distance probe:")
    print(f"   poetry run python scripts/train_probe.py -m experiment={experiment_group}/{model_name_sanitized}/dist/L0,L1,L2,...")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate a full suite of Hydra config files for a probing experiment sweep."
    )
    # --- FIX: Changed default for sanitized name to be filesystem-safe ---
    parser.add_argument("--model_hf_name", type=str, default="bert-base-multilingual-cased", help="Hugging Face model identifier.")
    parser.add_argument("--model_name_sanitized", type=str, default="bert-base-multilingual-cased", help="Filesystem-friendly name for the model.")
    # --- END FIX ---
    parser.add_argument("--dataset_name", type=str, default="ud_hindi_hdtb", help="Name of the dataset config file (without .yaml).")
    parser.add_argument("--experiment_group", type=str, default="baselines_hindi", help="Top-level folder name for the experiments under configs/experiment.")
    parser.add_argument("--model_dimension", type=int, default=768, help="Hidden dimension size of the model embeddings.")
    parser.add_argument("--num_layers", type=int, default=13, help="Total number of layers to generate configs for (embedding layer 0 + N transformer layers).")
    
    args = parser.parse_args()

    create_config_files(
        model_hf_name=args.model_hf_name,
        model_name_sanitized=args.model_name_sanitized.replace('/', '-'), # <-- ADDED SANITIZATION STEP
        dataset_name=args.dataset_name,
        experiment_group=args.experiment_group,
        model_dimension=args.model_dimension,
        num_layers=args.num_layers,
    )