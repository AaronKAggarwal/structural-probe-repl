# scripts/generate_configs.py
import argparse
import os
from pathlib import Path

def discover_available_languages():
    """
    Discover available languages by scanning the configs/dataset directory.
    Returns a dict mapping language names to their dataset config names.
    """
    dataset_dir = Path("configs/dataset")
    if not dataset_dir.exists():
        return {}
    
    languages = {}
    for dataset_path in dataset_dir.iterdir():
        if dataset_path.is_dir():
            dataset_name = dataset_path.name
            # Look for a config file with the same name as the directory
            expected_config = dataset_path / f"{dataset_name}.yaml"
            if expected_config.exists():
                # Infer language from dataset name (e.g., ud_hindi_hdtb -> hindi, ud_ewt -> english)
                if "hindi" in dataset_name.lower():
                    languages["hindi"] = dataset_name
                elif "ewt" in dataset_name.lower() or "english" in dataset_name.lower():
                    languages["english"] = dataset_name
                else:
                    # For other datasets, use a simplified name
                    lang_name = dataset_name.replace("ud_", "").split("_")[0]
                    languages[lang_name] = dataset_name
    
    return languages

def show_available_languages():
    """Display available languages and their corresponding dataset configs."""
    languages = discover_available_languages()
    if not languages:
        print("No language datasets found in configs/dataset/")
        print("Expected structure: configs/dataset/{dataset_name}/{dataset_name}.yaml")
        return
    
    print("Available languages:")
    for lang, dataset in languages.items():
        print(f"  {lang}: {dataset}")
    print()

def create_config_files(
    model_hf_name: str,
    model_name_sanitized: str,
    dataset_name: str,
    experiment_group: str,
    model_dimension: int,
    num_layers: int,
    base_output_dir: str = "configs",
):
    """
    Generates a full suite of Hydra config files for probing a model.

    This will create:
    1. An `embeddings` config for each layer.
    2. A `logging` config for each layer and probe type.
    3. An `experiment` config for each layer and probe type.
    """
    print(f"--- Generating configs for model '{model_hf_name}' ---")
    print(f"Dataset: {dataset_name}, Experiment Group: {experiment_group}")
    print(f"Layers: 0 to {num_layers - 1}, Dimension: {model_dimension}")
    print("-" * 50)

    base_path = Path(base_output_dir)

    # --- FIX: Use model_name_sanitized consistently for all directory paths ---
    embeddings_dir = base_path / "embeddings" / dataset_name / model_name_sanitized
    logging_dir = base_path / "logging" / f"{dataset_name}_{model_name_sanitized}"
    experiment_dir = base_path / "experiment" / experiment_group / model_name_sanitized # This line is the fix
    # --- END FIX ---

    # Create directories if they don't exist
    embeddings_dir.mkdir(parents=True, exist_ok=True)
    logging_dir.mkdir(parents=True, exist_ok=True)
    (experiment_dir / "dist").mkdir(parents=True, exist_ok=True)
    (experiment_dir / "depth").mkdir(parents=True, exist_ok=True)
    
    # Path template for HDF5 files (matches extract_embeddings.py default output)
    hdf5_path_template = f"data_staging/embeddings/{dataset_name}/{model_name_sanitized}/{dataset_name}_conllu_{{split}}_layers-all_align-mean.hdf5"

    for layer_idx in range(num_layers):
        layer_name = f"L{layer_idx}"

        # --- 1. Create Embeddings Config ---
        emb_path = embeddings_dir / f"{layer_name}.yaml"
        emb_content = f"""
# Auto-generated by generate_configs.py
source_model_name: "{model_hf_name}"
layer_index: {layer_idx}
dimension: {model_dimension}
paths:
  train: "{hdf5_path_template.format(split='train')}"
  dev: "{hdf5_path_template.format(split='dev')}"
  test: "{hdf5_path_template.format(split='test')}"
""".strip()
        with open(emb_path, "w") as f:
            f.write(emb_content)
        print(f"Created: {emb_path}")

        # --- 2. Create Configs for each Probe Type ---
        # Use shortened name "dist" to match existing folder/file conventions
        for probe_type in ["dist", "depth"]:
            
            probe_rank = model_dimension if probe_type == "depth" else 128

            # Use full probe name for human-readable fields (tags, probe config reference)
            probe_full_name = "distance" if probe_type == "dist" else "depth"
            
            # --- 2a. Create Logging Config ---
            log_path = logging_dir / f"{probe_type}_{layer_name}.yaml"
            log_content = f"""
# Auto-generated by generate_configs.py
wandb:
  tags: ["{experiment_group}", "{dataset_name}", "{model_name_sanitized}", "layer${{embeddings.layer_index}}", "{probe_full_name}", "r${{probe.rank}}"]
""".strip()
            with open(log_path, "w") as f:
                f.write(log_content)
            print(f"Created: {log_path}")

            # --- 2b. Create Experiment Config ---
            exp_path = experiment_dir / probe_type / f"{layer_name}.yaml"
            
            depth_overrides = ""
            if probe_type == "depth":
                depth_overrides = """
evaluation:
  metrics: ["spearmanr_hm", "root_acc"]
""".strip()

            exp_name_suffix = f"{probe_type.upper()}_R{probe_rank}"
            
            exp_content = f"""
# Auto-generated by generate_configs.py
name: {experiment_group}/{model_name_sanitized}/{probe_type}/{layer_name}

defaults:
  - _self_
  - /dataset: {dataset_name}/{dataset_name}
  - /embeddings: {dataset_name}/{model_name_sanitized}/{layer_name}
  - /probe: {probe_full_name}
  - /probe_rank@probe: "{probe_rank}"
  - /training: adam_modern_scheduler
  - /evaluation: default
  - /runtime: mps
  - /logging: {dataset_name}_{model_name_sanitized}/{probe_type}_{layer_name}

{depth_overrides}

logging:
  experiment_name: "{dataset_name.upper()}_{model_name_sanitized.upper()}_{layer_name}_{exp_name_suffix}"
""".strip()
            with open(exp_path, "w") as f:
                f.write(exp_content)
            print(f"Created: {exp_path}")
        print("-" * 20)

    print(f"\n--- Generation Complete for {num_layers} layers ---")
    # ... (rest of the script is unchanged) ...
    print("\nNext Steps:")
    print("1. Extract the embeddings for the model:")
    print(f"   poetry run python scripts/extract_embeddings.py dataset={dataset_name}/{dataset_name} model={model_name_sanitized} job.layers_to_extract=all")
    print("\n2. Run a sweep over all layers for the distance probe:")
    print(f"   poetry run python scripts/train_probe.py -m experiment={experiment_group}/{model_name_sanitized}/dist/L0,L1,L2,...")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate a full suite of Hydra config files for a probing experiment sweep."
    )
    
    # Show available languages first
    show_available_languages()
    
    # Required language parameter
    parser.add_argument("--language", type=str, required=True,
                        help="Language to generate configs for. Use 'list' to see available languages, or 'custom' for manual specification.")
    
    # Model parameters
    parser.add_argument("--model_hf_name", type=str, default="bert-base-multilingual-cased", help="Hugging Face model identifier.")
    parser.add_argument("--model_name_sanitized", type=str, default="bert-base-multilingual-cased", help="Filesystem-friendly name for the model.")
    parser.add_argument("--model_dimension", type=int, default=768, help="Hidden dimension size of the model embeddings.")
    parser.add_argument("--num_layers", type=int, default=13, help="Total number of layers to generate configs for (embedding layer 0 + N transformer layers).")
    
    # Optional overrides
    parser.add_argument("--dataset_name", type=str, default=None, help="Override dataset name (for custom language or non-standard naming).")
    parser.add_argument("--experiment_group", type=str, default=None, help="Override experiment group name (for custom organization).")
    
    args = parser.parse_args()

    # Handle special cases
    if args.language == "list":
        print("Use one of the available languages shown above.")
        exit(0)
    
    # Discover available languages
    available_languages = discover_available_languages()
    
    if args.language == "custom":
        if not args.dataset_name or not args.experiment_group:
            print("ERROR: When using --language custom, you must specify both --dataset_name and --experiment_group")
            exit(1)
        dataset_name = args.dataset_name
        experiment_group = args.experiment_group
    else:
        if args.language not in available_languages:
            print(f"ERROR: Language '{args.language}' not found.")
            print("Available languages:", list(available_languages.keys()))
            print("Use --language custom with --dataset_name and --experiment_group for manual specification.")
            exit(1)
        
        # Auto-determine dataset and experiment group
        dataset_name = args.dataset_name or available_languages[args.language]
        experiment_group = args.experiment_group or f"baselines_{args.language}"

    create_config_files(
        model_hf_name=args.model_hf_name,
        model_name_sanitized=args.model_name_sanitized.replace('/', '-'),
        dataset_name=dataset_name,
        experiment_group=experiment_group,
        model_dimension=args.model_dimension,
        num_layers=args.num_layers,
    )