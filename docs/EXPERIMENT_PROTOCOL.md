
# Experiment Protocol: Modern Structural Probe

Last updated: 2025-05-28 *(Updated to reflect current pipeline)*

This document outlines the standard procedures for configuring, running, and managing experiments with the modern structural probe implemented in `scripts/train_probe.py`, and the prerequisite embedding extraction using `scripts/extract_embeddings.py`. The framework uses [Hydra](https://hydra.cc/) for flexible and powerful configuration management.

## 1. Overview of Experimental Workflow

The typical workflow involves two main stages:

1.  **Stage 1: Embedding Extraction (using `scripts/extract_embeddings.py`)**
    *   **Goal:** Generate word-aligned hidden state embeddings from a target Language Model (e.g., BERT, Llama) for a given syntactically annotated dataset (e.g., Penn Treebank in CoNLL-X Stanford Dependencies format).
    *   **Inputs:**
        *   CoNLL-X/U files (train, dev, test splits).
        *   Hugging Face model identifier.
        *   Configuration for layers to extract and alignment strategy.
    *   **Output:** HDF5 files containing word-aligned embeddings.
    *   This is a prerequisite for probing new models or new datasets.

2.  **Stage 2: Probe Training & Evaluation (using `scripts/train_probe.py`)**
    *   **Goal:** Train a structural probe (distance or depth) on the pre-computed embeddings and evaluate its performance against gold syntactic structures.
    *   **Inputs:**
        *   CoNLL-X/U files (for gold syntactic structures: heads, XPOS tags).
        *   HDF5 files of pre-computed word-aligned embeddings.
        *   Configuration for probe type, rank, training hyperparameters, and evaluation.
    *   **Output:** Trained probe model checkpoints, detailed metrics, summary reports, plots, and (optionally) Weights & Biases logs.

This protocol primarily details the configuration and execution of these two scripts.

## 2. Prerequisites

*   **Environment:**
    *   Native macOS environment (with MPS) or a Linux environment with CUDA GPUs.
    *   Environment set up as per `docs/ENV_SETUP.md`.
    *   Poetry environment must be active (e.g., via `poetry shell` or by prefixing commands with `poetry run`).
    *   For PTB processing: Stanford CoreNLP 3.9.2 and Java installed as per project setup.
*   **Data:**
    *   **Gold Syntactic Parses:** CoNLL-X files (e.g., PTB Stanford Dependencies from `data_processing_scripts/ptb_to_conllx.sh`) or CoNLL-U files for train, development, and (optionally) test splits. These files must be parseable by `src/torch_probe/utils/conllu_reader.py` (expecting XPOS tags for H&M alignment).
    *   **Pre-computed Embeddings (for Stage 2):** HDF5 files containing word-aligned hidden state embeddings for the corresponding CoNLL splits and target LLM layer, generated by `scripts/extract_embeddings.py`.
*   **Configuration Files:** Familiarity with the Hydra configuration structure in the `configs/` directory. See `docs/ARCHITECTURE.md` for an overview of config file organization.

## 3. Configuration Management with Hydra

All experiments for both `extract_embeddings.py` and `train_probe.py` are controlled via YAML configuration files managed by Hydra.

*   **Main Configurations:**
    *   `configs/config_extract.yaml`: Base configuration for embedding extraction.
    *   `configs/config.yaml`: Base configuration for probe training and evaluation.
*   **Configuration Groups:** Located in subdirectories under `configs/` (e.g., `configs/dataset/`, `configs/embeddings/`, `configs/probe/`, `configs/training/`, `configs/evaluation/`). These files define specific options for each component.
*   **Experiment Files (for `train_probe.py`):** Located in `configs/experiment/`. These files compose configurations from different groups to define a complete probing experiment (e.g., `hm_replication/bert_base_L7_dist_ptb_sd.yaml`).

**Key Configurable Parameters (Illustrative - see specific base configs for full structure):**

*   **For `extract_embeddings.py` (via `configs/config_extract.yaml`):**
    *   `input_conll_files.{train|dev|test|sample_dev}`: Paths to input CoNLL-X/U files.
    *   `model.hf_model_name`: Hugging Face model identifier.
    *   `model.layers_to_extract`: List of layer indices or "all".
    *   `model.alignment_strategy`: "mean" or "first".
    *   `output_hdf5.base_output_path`: Directory to save HDF5 embedding files.
    *   `runtime.device`.

*   **For `train_probe.py` (via `configs/config.yaml` and experiment files):**
    *   `dataset.paths.conllu_{train|dev|test}`: Paths to CoNLL-X/U files (for gold structures).
    *   `embeddings.paths.{train|dev|test}`: Paths to HDF5 embedding files.
    *   `embeddings.layer_index`: Which layer from the HDF5 stack to use for probing.
    *   `embeddings.dimension`: Dimensionality of input embeddings.
    *   `probe.type`: "distance" or "depth".
    *   `probe.rank`: Dimensionality of the probe's projection.
    *   `evaluation.metrics`: List, e.g., `["loss", "spearmanr_hm", "uuas", "root_acc"]`.
    *   `evaluation.spearman_min_len`, `evaluation.spearman_max_len`: For H&M style Spearman.
    *   `training.optimizer.{name|lr|betas|eps|weight_decay}`.
    *   `training.batch_size`, `training.epochs`, `training.patience` (for early stopping).
    *   `training.early_stopping_metric`, `training.early_stopping_delta`.
    *   `training.lr_scheduler_with_reset.{enable|lr_decay_factor|lr_decay_patience|min_lr}`.
    *   `training.save_every_epoch_checkpoint` (true/false).
    *   `training.save_checkpoint_every_n_epochs` (-1 or N, for periodic saves).
    *   `training.eval_on_train_every_n_epochs`: (e.g., 10 or -1) Frequency to evaluate on the training set (use -1 to only eval at end).
    *   `runtime.device`, `runtime.seed`, `runtime.num_workers`.
    *   `logging.experiment_name`.
    *   `logging.enable_plots`.
    *   `logging.wandb.{enable|project|entity|notes|tags|...}`.

## 4. Stage 1: Running Embedding Extraction (`scripts/extract_embeddings.py`)

1.  **Prepare/Verify `configs/config_extract.yaml`:** Ensure default paths and model parameters are sensible or plan to override them. Key sections: `input_conll_files`, `model`, `output_hdf5`, `runtime`.
2.  **Execute Script:**
    ```bash
    # Example: Extract all layers for bert-base-cased using defaults from config_extract.yaml
    poetry run python scripts/extract_embeddings.py

    # Example: Override for a specific model and layers, processing only a sample dev file
    poetry run python scripts/extract_embeddings.py \
      input_conll_files.sample_dev="data/ptb_stanford_dependencies_conllx/ptb3-wsj-TINY_SAMPLE.conllx" \
      input_conll_files.train=null \
      input_conll_files.dev=null \
      input_conll_files.test=null \
      model.hf_model_name="roberta-base" \
      model.layers_to_extract="[0,6,12]" \
      model.alignment_strategy="mean" \
      output_hdf5.base_output_path="data/embeddings_roberta_sample" \
      runtime.device="mps"
    ```
3.  **Outputs:** HDF5 files containing word-aligned embeddings will be saved in the directory specified by `output_hdf5.base_output_path` (e.g., `data/embeddings_ptb_sd/bert-base-cased_train_layers-all_align-mean.hdf5`). Logs for the extraction script itself will be in Hydra's output directory (e.g., `outputs_extract_embeddings/...`).

## 5. Stage 2: Running Probe Training & Evaluation (`scripts/train_probe.py`)

### 5.1. Learning Rate Scheduling (H&M Style Optimizer Reset)

The script supports an H&M-style LR decay with optimizer reset, configured via `training.lr_scheduler_with_reset.*` (see parameters in Section 3). When enabled and dev metric (e.g., loss) does not improve for `lr_decay_patience` epochs, the LR is decayed, and the optimizer and early stopper states are reset. For H&M replication, `lr_decay_patience: 1` is typically used.

### 5.2. Checkpointing

Controlled by `training.save_every_epoch_checkpoint` and `training.save_checkpoint_every_n_epochs`.
*   If `is_best_for_checkpoint` is true (based on `early_stopping_metric`), a checkpoint for that epoch and `_best.pt` are saved.
*   If `save_every_epoch_checkpoint=true`, an epoch-specific checkpoint is saved every epoch.
*   If `save_checkpoint_every_n_epochs=N` (N>0), an epoch-specific checkpoint is saved every Nth epoch.
These conditions are OR-ed: if any are true, the epoch checkpoint is saved.

### 5.3. Running a Single Experiment

**Command Structure:**
```bash
poetry run python scripts/train_probe.py experiment=<path/to/experiment_file_name_without_yaml> [OTHER_OVERRIDES...]
```
*   **Example: Running an H&M Replication Experiment (BERT-base L7 Distance):**
    (Assumes `configs/experiment/hm_replication/bert_base_L7_dist_ptb_sd.yaml` and its dependencies exist)
    ```bash
    poetry run python scripts/train_probe.py \
        experiment=hm_replication/bert_base_L7_dist_ptb_sd \
        logging.wandb.enable=true \
        logging.wandb.project="structural-probes-hm-replication" \
        logging.wandb.entity="your_wandb_entity" \
        training.eval_on_train_every_n_epochs=-1 # Example: only eval train at end
    ```

*   **Overriding Parameters from Command Line (Full Override Example):**
    If not using an experiment file, or to override many defaults from `configs/config.yaml`:
    ```bash
    poetry run python scripts/train_probe.py \
      dataset=ptb_sd_official \
      embeddings=bert_base_L7_ptb_sd \
      probe=distance_probe_bert_base_fullrank \
      training=training_hm_replication \
      evaluation=eval_hm_metrics \
      "logging.experiment_name=My_BERT_L7_Run" \
      logging.wandb.enable=true \
      # ... other specific overrides ...
    ```

### 5.4. Running Multiple Experiments (Sweeps with Hydra Multirun)

Use `-m` or `--multirun` to sweep over parameters.
*   **Example: Sweeping BERT-base layers for a distance probe:**
    (Assumes `configs/experiment/hm_replication/bert_base_dist_ptb_sd_LAYER_SWEEP_BASE.yaml` is a base config where `embeddings.layer_index` can be overridden, and `configs/embeddings/bert_base_ALL_LAYERS_ptb_sd.yaml` points to an HDF5 with all layers).
    ```bash
    poetry run python scripts/train_probe.py -m \
      experiment=hm_replication/bert_base_dist_ptb_sd_LAYER_SWEEP_BASE \
      embeddings.layer_index=0,1,2,3,4,5,6,7,8,9,10,11,12 \
      hydra.job.name="hm_repl_bert_base_L\${embeddings.layer_index}_dist" \
      "logging.experiment_name=hm_repl_bert_base_L\${embeddings.layer_index}_dist_ptb_sd" \
      logging.wandb.tags="['hm_replication','bert-base','distance','ptb_sd','layer_sweep']"
    ```

### 5.5. Controlling Hydra's Output Directory

*   Hydra saves outputs to a unique timestamped directory (e.g., `outputs/YYYY-MM-DD/HH-MM-SS/` for single runs, `multirun/...` for sweeps). This is configured in `configs/config.yaml` via `hydra.run.dir` and `hydra.sweep.dir`.
*   The script `train_probe.py` uses `Path(HydraConfig.get().runtime.output_dir)` to robustly determine this path for saving all its artifacts.

## 6. Interpreting Outputs from `train_probe.py`

Each run creates a unique output directory containing:
*   **`.hydra/`:** Resolved configuration files (`config.yaml`, `hydra.yaml`, `overrides.yaml`).
*   **`train_probe.log`:** Main log file from the script.
*   **`checkpoints/`:**
    *   `<probe_type>_probe_rank<X>_epoch<N>_metric<Y>.pt`: Saved based on `is_best`, `save_every_epoch_checkpoint`, or `save_checkpoint_every_n_epochs`.
    *   `<probe_type>_probe_rank<X>_best.pt`: Checkpoint with the best dev set performance on `training.early_stopping_metric`.
*   **`metrics_summary.json`:** Key final metrics (best dev score, epochs completed, test set scores).
*   **`dev_detailed_metrics_epochN.json` / `train_detailed_metrics_epochN.json`:** Per-epoch detailed metrics (per-sentence scores, etc.) if saved.
*   **`test_detailed_metrics_final.json`:** Detailed metrics for the test set evaluation.
*   **Plot images (e.g., `loss_vs_epoch.png`, `dev_{monitor_metric}_vs_epoch.png`)** if `logging.enable_plots=true`.
*   **`wandb/` subdirectory:** Contains local W&B files if W&B logging is enabled.

## 7. Weights & Biases Integration

*   Controlled by `logging.wandb.*` config settings.
*   If `logging.wandb.enable=true`:
    *   Logs hyperparameters (full Hydra config).
    *   Logs metrics (train/dev/test loss, spearmanr_hm, uuas, root_acc) over epochs.
    *   Logs learning rate.
    *   Optionally logs model checkpoints, detailed metric JSONs, and plots as W&B Artifacts and Images.
*   Ensure `wandb login` has been performed.

## 8. Adding New Datasets, Embeddings, or Probe Configurations

Follow these general steps:
1.  **New Dataset:**
    *   Prepare CoNLL-X/U files.
    *   Create a YAML file in `configs/dataset/` (e.g., `my_new_dataset.yaml`) defining its `name` and `paths`.
2.  **New Embeddings:**
    *   Run `scripts/extract_embeddings.py` with appropriate configuration to generate HDF5 files for your new model/dataset.
    *   Create a YAML file in `configs/embeddings/` (e.g., `my_model_layerX_on_my_dataset.yaml`) pointing to these HDF5s and specifying `layer_index` (for `ProbeDataset` to select from the HDF5 stack) and `dimension`.
3.  **New Probe/Training/Evaluation Config:**
    *   Create YAML files in `configs/probe/`, `configs/training/`, or `configs/evaluation/` if different parameters are needed.
4.  **New Experiment File:**
    *   Create a YAML file in `configs/experiment/` (e.g., `probe_my_model_on_my_dataset.yaml`).
    *   In its `defaults` list, use `override /group: choice` to select your new dataset, embedding, probe, training, and evaluation configs.
    *   Define experiment-specific logging (e.g., `logging.experiment_name`, W&B tags).
5.  **Run:**
    ```bash
    poetry run python scripts/train_probe.py experiment=path/to/your/new_experiment_file
    ```

This protocol provides a framework for systematic and reproducible experimentation. Always refer to specific `.yaml` files for detailed parameter names and structures.
```

---

**Key Updates and Integrations in this Version:**

*   **Structure:** Clearly separated into Stage 1 (Embedding Extraction) and Stage 2 (Probe Training).
*   **Prerequisites:** Added note about Stanford CoreNLP/Java for PTB. Specified XPOS tags expectation.
*   **Configuration:** Mentions `config_extract.yaml` for Stage 1. Includes new parameters we added (checkpointing, Spearman lengths, `eval_on_train_every_n_epochs`).
*   **Stage 1 Section:** Added a dedicated section for running `scripts/extract_embeddings.py` with examples.
*   **Stage 2 - Probe Training:**
    *   Re-iterated H&M LR scheduler and new checkpointing logic.
    *   Updated example run commands for clarity.
    *   Corrected an example for layer sweep to reflect how `embeddings.layer_index` works with HDF5s that might contain multiple layers.
*   **Interpreting Outputs:** Clarified which detailed metrics files are per-epoch.
*   **General Polish:** Ensured consistency and clarity throughout.

This should be a very comprehensive guide to run experiments with the current pipeline.