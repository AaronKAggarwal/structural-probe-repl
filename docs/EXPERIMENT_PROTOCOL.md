# Experiment Protocol: Modern Structural Probe

Last updated: 2025-05-26

This document outlines the standard procedures for configuring, running, and managing experiments with the modern structural probe implemented in `scripts/train_probe.py`. The framework uses [Hydra](https://hydra.cc/) for flexible and powerful configuration management.

## 1. Overview

The modern probing pipeline consists of two main stages that might be run:

1.  **Embedding Extraction (Phase 2 Task):** Generating word-aligned hidden state embeddings from various LLMs for a given dataset (e.g., PTB). This is a prerequisite for probing new models and is handled by `scripts/extract_embeddings.py` (to be developed in Phase 2).
2.  **Probe Training & Evaluation (Phase 1 Deliverable):** Training a structural probe (distance or depth) on pre-computed embeddings and evaluating its performance. This is handled by `scripts/train_probe.py`.

This protocol primarily focuses on **Stage 2: Probe Training & Evaluation**.

## 2. Prerequisites

*   **Environment:** Native macOS environment (with MPS) or the CUDA Docker environment (`probe:cuda` on a GPU machine) set up as per `docs/ENV_SETUP.md`. Poetry environment must be active (e.g., via `poetry shell` or by prefixing commands with `poetry run`).
*   **Data:**
    *   **Gold Syntactic Parses:** CoNLL-U files for train, development, and (optionally) test splits of your target treebank (e.g., PTB, or the EWT sample for initial tests).
    *   **Pre-computed Embeddings:** HDF5 files (or other supported format) containing word-aligned hidden state embeddings for the corresponding CoNLL-U splits and target LLM layer. These should be generated by the embedding extraction pipeline (Phase 2).
*   **Configuration Files:** Familiarity with the Hydra configuration structure in the `configs/` directory.

## 3. Configuration Management with Hydra

All experiments are controlled via YAML configuration files managed by Hydra.

*   **Main Configuration:** `configs/config.yaml` defines the overall structure and default choices for various configuration groups.
*   **Configuration Groups:** Located in subdirectories under `configs/` (e.g., `configs/dataset/`, `configs/embeddings/`, `configs/probe/`, `configs/training/`). These files define specific options for each component.
*   **Experiment Files:** Located in `configs/experiment/`. These files compose configurations from different groups to define a complete experiment. For example, `elmo_ewt_dist_phase1_train.yaml` selects a dataset, embedding set, probe type, and training setup.

**Key Configurable Parameters (see `configs/config.yaml` for full structure):**

*   `dataset.paths.conllu_{train|dev|test}`
*   `embeddings.paths.{train|dev|test}`
*   `embeddings.layer_index` (which layer from the HDF5 to use)
*   `embeddings.dimension` (of the input embeddings)
*   `probe.type` ("distance" or "depth")
*   `probe.rank` (dimensionality of the probe's projection)
*   `training.optimizer.{name|lr|weight_decay|...}`
*   `training.batch_size`, `training.epochs`, `training.patience`
*   `training.early_stopping_metric` (e.g., "loss", "uuas", "spearmanr", "root_acc")
*   `runtime.device` ("cpu", "mps", "cuda", "auto")
*   `runtime.seed`
*   `logging.experiment_name`
*   `logging.wandb.enable` (true/false)


## 4. Learning Rate Scheduling (H&M Style Optimizer Reset)

The `train_probe.py` script supports an optional learning rate decay mechanism with optimizer state reset, similar to that described by Hewitt & Manning. This is configured under the `training.lr_scheduler_with_reset` group in your Hydra configuration (e.g., `configs/training/default_adam.yaml`).

Key parameters:
*   `enable: true` (boolean): Set to `true` to activate this scheduler. Defaults to `false`.
*   `lr_decay_factor: 0.1` (float): Factor by which the current learning rate is multiplied.
*   `lr_decay_patience: 3` (int): Number of validation epochs without improvement in the `training.early_stopping_metric` before the LR is decayed.
*   `min_lr: 1.0e-6` (float): The learning rate will not be decayed below this value.

When the LR is decayed:
1.  The optimizer is re-initialized with the new, lower learning rate (this resets its internal state, like momentum).
2.  The patience counter for the *LR scheduler itself* is reset.
3.  The `best_actual_metric` for the *main `EarlyStopper`* (for overall training termination) is also reset to give the model a chance to improve with the new LR.

**Example Override to Enable and Configure:**
```bash
poetry run python scripts/train_probe.py experiment=... \
    training.lr_scheduler_with_reset.enable=true \
    training.lr_scheduler_with_reset.lr_decay_patience=2 \
    training.lr_scheduler_with_reset.lr_decay_factor=0.5
```

## 5. Running a Single Experiment

The main script for training and evaluating probes is `scripts/train_probe.py`.

**Command Structure:**
```bash
poetry run python scripts/train_probe.py [GROUP_OVERRIDES...] [PARAM_OVERRIDES...] [HYDRA_OVERRIDES...]
```

*   **Selecting an Experiment File:**
    ```bash
    poetry run python scripts/train_probe.py experiment=<experiment_file_name>
    ```
    Example:
    ```bash
    poetry run python scripts/train_probe.py experiment=elmo_ewt_dist_phase1_train
    ```
    This will load `configs/config.yaml` and then compose it with `configs/experiment/elmo_ewt_dist_phase1_train.yaml`.

*   **Overriding Parameters from Command Line:**
    Any parameter in the configuration can be overridden.
    ```bash
    poetry run python scripts/train_probe.py \
        experiment=elmo_ewt_dist_phase1_train \
        training.epochs=50 \
        training.optimizer.lr=0.0005 \
        probe.rank=64 \
        runtime.device=mps \
        logging.wandb.enable=true \
        logging.experiment_name="elmo_dist_rank64_lr5e-4_mps"
    ```

*   **Controlling Hydra's Output Directory:**
    By default, Hydra saves outputs to `outputs/YYYY-MM-DD/HH-MM-SS/`.
    To specify a custom directory for a run:
    ```bash
    poetry run python scripts/train_probe.py \
        experiment=elmo_ewt_dist_phase1_train \
        hydra.run.dir=my_custom_outputs/my_run_001
    ```

## 6. Running Multiple Experiments (Sweeps with Hydra Multirun)

Hydra's multirun feature is powerful for hyperparameter sweeps or running across multiple configurations (e.g., different layers).

**Command Structure:**
```bash
poetry run python scripts/train_probe.py --multirun [PARAMS_TO_SWEEP...] [OTHER_OVERRIDES...]
# Short form: poetry run python scripts/train_probe.py -m ...
```

*   **Example: Sweeping Probe Rank and Learning Rate for an Experiment:**
    ```bash
    poetry run python scripts/train_probe.py -m \
        experiment=elmo_ewt_dist_phase1_train \
        probe.rank=32,64,128 \
        training.optimizer.lr=0.001,0.0005 \
        runtime.device=mps \
        logging.wandb.enable=true 
    ```
    This will launch 3 (ranks) * 2 (LRs) = 6 individual jobs.

*   **Example: Sweeping through Embedding Layers (assuming HDF5 stores multiple layers):**
    If your `ProbeDataset` and `embedding_loader` are set up to select a layer from a multi-layer HDF5 file based on `embeddings.layer_index`:
    ```bash
    # Assume elmo_ewt_dist_phase1_train.yaml points to an HDF5 with all ELMo layers
    poetry run python scripts/train_probe.py -m \
        experiment=elmo_ewt_dist_phase1_train \
        embeddings.layer_index=0,1,2 \
        runtime.device=mps \
        logging.wandb.enable=true
    ```
    This will run the probe training three times, once for each ELMo layer.

*   **Output Directory for Multirun:**
    Hydra saves multirun outputs to `multirun/YYYY-MM-DD/HH-MM-SS/` by default, with subdirectories for each job (0, 1, 2,...).

## 7. Interpreting Outputs

For each individual run (whether single or part of a multirun), Hydra creates an output directory. Inside this directory:

*   **`.hydra/`:**
    *   `config.yaml`: The fully resolved configuration used for this specific run.
    *   `hydra.yaml`: Hydra's internal configuration for the run.
    *   `overrides.yaml`: The command-line overrides applied.
*   **`train_probe.log` (or similar):** Standard log output from the `scripts/train_probe.py` script (if Python logging is configured and captured by Hydra).
*   **`checkpoints/`:**
    *   `<probe_type>_probe_rank<X>_epoch<N>_metric<Y>.pt`: Checkpoints saved periodically during training.
    *   `<probe_type>_probe_rank<X>_best.pt`: The checkpoint corresponding to the best performance on the development set according to the `training.early_stopping_metric`.
*   **`metrics_summary.json`:** A JSON file containing key metrics, such as:
    *   `best_dev_monitored_metric_value`: The best value of the metric monitored for early stopping.
    *   `test_loss`, `test_spearmanr`, `test_uuas`, `test_root_acc`: Metrics from evaluating the best model on the (optional) test set.

## 8. Weights & Biases Integration (Optional)

If `logging.wandb.enable=true` in the configuration:
*   Metrics (training loss, dev loss, dev metrics, test metrics) will be logged to the specified W&B project.
*   The full Hydra configuration for the run will also be logged to W&B.
*   This allows for easy visualization, comparison across runs, and collaboration.
*   Ensure you have `wandb` installed (`poetry add wandb`) and are logged in (`wandb login`).

## 9. Adding New Datasets, Embeddings, or Probe Configurations

1.  **New Dataset:**
    *   Prepare CoNLL-U files (train, dev, test).
    *   Create a YAML file in `configs/dataset/` (e.g., `my_new_dataset.yaml`) defining its `name` and `paths`.
2.  **New Embeddings (for an existing or new dataset):**
    *   Run the embedding extraction pipeline (`scripts/extract_embeddings.py` - Phase 2 deliverable) to generate HDF5 files.
    *   Create a YAML file in `configs/embeddings/` (e.g., `my_model_layerX_on_my_dataset.yaml`) defining `source_model_name`, `layer_index`, `paths` to HDF5s, and `dimension`.
3.  **New Probe Configuration (e.g., different rank):**
    *   Create a YAML file in `configs/probe/` (e.g., `distance_rank64.yaml`) defining `type` and `rank`.
4.  **New Experiment File:**
    *   Create a YAML file in `configs/experiment/` (e.g., `probe_my_model_on_my_dataset_dist_r64.yaml`).
    *   In its `defaults` list, use `override` to select your new dataset, embedding, and probe configs, along with a training config.
    *   Add any experiment-specific overrides.
5.  **Run:**
    ```bash
    poetry run python scripts/train_probe.py experiment=probe_my_model_on_my_dataset_dist_r64
    ```

This protocol should allow for systematic and reproducible experimentation. Refer to `docs/ARCHITECTURE.md` for details on code modules and `docs/CONFIG_STRUCTURE.md` (if created, or this section) for more on Hydra configuration.


**Notes on this `EXPERIMENT_PROTOCOL.md`:**

*   **Focus on Modern Probe:** It's tailored for `scripts/train_probe.py`.
*   **Assumes Phase 2 Deliverables:** It refers to `scripts/extract_embeddings.py` which is a Phase 2 task. You can add a note that this script is forthcoming.
*   **Hydra Centric:** Emphasizes the role of Hydra.
*   **Evolvable:** This document will evolve as the framework gets more features (e.g., more complex embedding types, different probe architectures).
