# Experiment Protocol: Modern Structural Probe

Last updated: 2025-06-17

This document outlines the standard procedures for configuring, running, and managing experiments with the modern structural probe implemented in `scripts/train_probe.py`, and the prerequisite embedding extraction using `scripts/extract_embeddings.py`. The framework uses [Hydra](https://hydra.cc/) for flexible and powerful configuration management.

The project has now pivoted to using **Universal Dependencies (UD)** treebanks as the primary evaluation corpus, though the pipeline remains compatible with the Penn Treebank (PTB) format used for the initial H&M replication.

## 1. Overview of Experimental Workflow

The typical workflow involves two main stages:

1.  **Stage 1: Embedding Extraction**
    *   **Goal:** Generate word-aligned hidden state embeddings from a target Language Model (e.g., BERT, Llama, ELMo) for a given syntactically annotated dataset.
    *   **Inputs:**
        *   CoNLL-U files (e.g., from a UD treebank) or CoNLL-X files (from PTB-SD).
        *   A Hugging Face model identifier for transformer models, or configuration for legacy models like ELMo.
        *   Configuration for layers to extract and alignment strategy.
    *   **Output:** HDF5 files containing word-aligned embeddings for each data split.
    *   **Key Scripts:**
        *   `scripts/extract_embeddings.py` (for Hugging Face models).
        *   `scripts/generate_elmo_embeddings_generic.sh` (for legacy ELMo).

2.  **Stage 2: Probe Training & Evaluation**
    *   **Goal:** Train a structural probe (distance or depth) on the pre-computed embeddings and evaluate its performance against gold syntactic structures.
    *   **Inputs:**
        *   The original CoNLL-U/X files (for gold syntactic structures: heads, POS tags).
        *   The HDF5 files of pre-computed embeddings from Stage 1.
        *   Configuration for probe type, rank, training hyperparameters, and evaluation.
    *   **Output:** Trained probe model checkpoints, detailed metrics, summary reports, plots, and (optionally) Weights & Biases logs.
    *   **Key Script:** `scripts/train_probe.py`.

## 2. Prerequisites

*   **Environment:**
    *   Native macOS environment (with MPS) or a Linux environment with CUDA GPUs.
    *   Environment set up as per `docs/ENV_SETUP.md`.
    *   Poetry environment must be active (e.g., via `poetry shell` or by prefixing commands with `poetry run`).
*   **Data:**
    *   **Gold Syntactic Parses:** Your primary dataset (e.g., UD English EWT) should be placed in the `data/` directory. The files must be parseable by `src/torch_probe/utils/conllu_reader.py`, which expects columns for tokens, heads, UPOS tags, and XPOS tags.
    *   **Pre-computed Embeddings (for Stage 2):** HDF5 files generated by the Stage 1 scripts, corresponding to the CoNLL splits and target LLM layer.
*   **Configuration Files:** Familiarity with the Hydra configuration structure in the `configs/` directory. See `docs/ARCHITECTURE.md` for an overview.

## 3. Configuration Management with Hydra

All experiments are controlled via YAML files in `configs/` managed by Hydra.

*   **Main Configurations:**
    *   `configs/config_extract.yaml`: Base configuration for transformer embedding extraction.
    *   `configs/extraction/`: Directory for specific, one-off extraction configs.
    *   `configs/config.yaml`: Base configuration for probe training and evaluation.
*   **Configuration Groups:** Located in subdirectories under `configs/` (e.g., `configs/dataset/`, `configs/embeddings/`, `configs/probe/`).
*   **Experiment Files:** Located in `configs/experiment/`. These compose configurations from different groups to define a complete probing experiment (e.g., `ud_replication/bert_base_L7_dist_ud_en_ewt.yaml`).

## 4. Stage 1: Running Embedding Extraction

### 4.1 For Hugging Face Transformer Models (e.g., BERT, Llama)

1.  **Prepare a Config:** Create a new YAML file in `configs/extraction/`, for example `configs/extraction/my_model_on_ud_ewt.yaml`, specifying the model, dataset paths, and output path. (See `configs/extraction/bert_base_cased_ud_ewt_all_layers.yaml` for an example).
2.  **Execute Script:** Run `extract_embeddings.py` by pointing it to your new config.
    ```bash
    poetry run python scripts/extract_embeddings.py --config-name=extraction/my_model_on_ud_ewt.yaml
    ```
3.  **Outputs:** HDF5 files will be saved in the directory specified by `output_hdf5.base_output_path`.

### 4.2 For Legacy ELMo

1.  **Convert CoNLL-U to Raw Text:** First, create a raw text file from your CoNLL-U data split.
    ```bash
    # Usage: python script.py <input_conllu> <output_txt>
    poetry run python scripts/convert_conllu_to_raw_generic.py \
      data/ud_english_ewt_official/en_ewt-ud-train.conllu \
      data_staging/ud_ewt_official_processed/raw_text/en_ewt-ud-train.txt
    ```
2.  **Generate ELMo Embeddings:** Run the generic shell script, providing the input raw text and desired output HDF5 path.
    ```bash
    # Usage: ./script.sh <input_txt> <output_hdf5>
    ./scripts/generate_elmo_embeddings_generic.sh \
      data_staging/ud_ewt_official_processed/raw_text/en_ewt-ud-train.txt \
      data_staging/ud_ewt_official_processed/elmo_hdf5_layers/en_ewt-ud-train.elmo-layers.hdf5
    ```

## 5. Stage 2: Running Probe Training & Evaluation

### 5.1. Punctuation Filtering in Evaluation

The evaluation metrics (UUAS, Spearman, RootAcc) are aligned with H&M's methodology, which ignores punctuation. **Crucially, the current implementation identifies punctuation using PTB-style XPOS tags.** This works correctly for both the PTB-SD dataset and the UD English EWT treebank (which also provides PTB-style XPOS tags). For other UD languages, this logic may need to be adapted to use the universal `UPOS == 'PUNCT'` tag.

### 5.2. Learning Rate Scheduling & Checkpointing

The training script supports an H&M-style LR decay with optimizer reset (`training.lr_scheduler_with_reset`) and granular checkpointing options, configured in your `training/*.yaml` or experiment files.

### 5.3. Running a Single Experiment on UD EWT

**Command Structure:**
```bash
poetry run python scripts/train_probe.py experiment=<path/to/experiment_file_name_without_yaml>
```
*   **Example: Running the ELMo Layer 1 Distance Probe Baseline on UD EWT:**
    (Assumes `configs/experiment/elmo_fullewt_dist_L1_probe.yaml` and its dependencies exist)
    ```bash
    poetry run python scripts/train_probe.py \
        experiment=elmo_fullewt_dist_L1_probe \
        logging.wandb.enable=true
    ```
*   **Example: Running the BERT Layer 7 Distance Probe Baseline on UD EWT:**
    ```bash
    poetry run python scripts/train_probe.py \
        experiment=bert_base_L7_dist_udewt_probe \
        logging.wandb.enable=true
    ```

### 5.4. Running Multiple Experiments (Sweeps with Hydra Multirun)

Use the `-m` or `--multirun` flag to sweep over parameters.

*   **Example: Sweeping ELMo layers for a distance probe on UD EWT:**
    (This requires a base experiment file, e.g., `elmo_fullewt_dist_LAYER_SWEEP_BASE.yaml`, where the `embeddings` group can be easily overridden).
    ```bash
    poetry run python scripts/train_probe.py -m \
      experiment=elmo_fullewt_dist_LAYER_SWEEP_BASE \
      embeddings=elmo_l0_ud_ewt_full,elmo_l1_ud_ewt_full,elmo_l2_ud_ewt_full \
      "logging.experiment_name=ud_ewt_elmo_L\${embeddings.layer_index}_dist"
    ```

## 6. Interpreting Outputs from `train_probe.py`

Each run creates a unique output directory containing:
*   **`.hydra/`:** Resolved configuration files (`config.yaml`, `hydra.yaml`, `overrides.yaml`).
*   **`train_probe.log`:** Main log file from the script.
*   **`checkpoints/`:**
    *   `<probe_type>_probe_rank<X>_epoch<N>_metric<Y>.pt`: Saved based on `is_best`, `save_every_epoch_checkpoint`, or `save_checkpoint_every_n_epochs`.
    *   `<probe_type>_probe_rank<X>_best.pt`: Checkpoint with the best dev set performance on `training.early_stopping_metric`.
*   **`metrics_summary.json`:** Key final metrics (best dev score, epochs completed, test set scores).
*   **`dev_detailed_metrics_epochN.json` / `train_detailed_metrics_epochN.json`:** Per-epoch detailed metrics (per-sentence scores, etc.).
*   **`test_detailed_metrics_final.json`:** Detailed metrics for the test set evaluation.
*   **Plot images** if `logging.enable_plots=true`.

## 7. Weights & Biases Integration

*   Controlled by `logging.wandb.*` config settings.
*   If `logging.wandb.enable=true`, the script will log hyperparameters, metrics, and optionally model artifacts. Ensure `wandb login` has been performed.

## 8. Adding New Datasets or Models

1.  **New Dataset:** Prepare CoNLL-U files, place them in `data/`, and create a new YAML file in `configs/dataset/` defining the paths.
2.  **New Embeddings:** Run the appropriate Stage 1 script (Section 4) to generate HDF5 files. Create a new YAML file in `configs/embeddings/` pointing to these files.
3.  **New Experiment:** Create a YAML file in `configs/experiment/` that composes your new dataset and embedding configs, along with probe and training settings.
4.  **Run:** Execute `scripts/train_probe.py` with your new experiment file.