# Experiment Protocol: Modern Structural Probe

Last updated: 2025-07-16

This document outlines the standard procedures for configuring, running, and managing experiments with the modern structural probe implemented in `scripts/train_probe.py`, and the prerequisite embedding extraction using `scripts/extract_embeddings.py`. The framework uses [Hydra](https://hydra.cc/) for flexible and powerful configuration management.

The project has now pivoted to using **Universal Dependencies (UD)** treebanks as the primary evaluation corpus, though the pipeline remains compatible with the Penn Treebank (PTB) format used for the initial H&M replication.

## 1. Overview of Experimental Workflow

The typical workflow involves two main stages:

1.  **Stage 1: Embedding Extraction**
    *   **Goal:** Generate word-aligned hidden state embeddings from a target Language Model (e.g., BERT, Llama, ELMo) for a given syntactically annotated dataset.
    *   **Inputs:**
        *   CoNLL-U files (e.g., from a UD treebank) or CoNLL-X files (from PTB-SD).
        *   A Hugging Face model identifier for transformer models, or configuration for legacy models like ELMo.
        *   Configuration for layers to extract and alignment strategy.
    *   **Output:** HDF5 files containing word-aligned embeddings for each data split.
    *   **Key Scripts:**
        *   `scripts/extract_embeddings.py` (for Hugging Face models).
        *   `scripts/generate_elmo_embeddings_generic.sh` (for legacy ELMo).

2.  **Stage 2: Probe Training & Evaluation**
    *   **Goal:** Train a structural probe (distance or depth) on the pre-computed embeddings and evaluate its performance against gold syntactic structures.
    *   **Inputs:**
        *   The original CoNLL-U/X files (for gold syntactic structures: heads, POS tags).
        *   The HDF5 files of pre-computed embeddings from Stage 1.
        *   Configuration for probe type, rank, training hyperparameters, and evaluation.
    *   **Output:** Trained probe model checkpoints, detailed metrics, summary reports, plots, and (optionally) Weights & Biases logs.
    *   **Key Script:** `scripts/train_probe.py`.

## 2. Prerequisites

*   **Environment:**
    *   Native macOS environment (with MPS) or a Linux environment with CUDA GPUs.
    *   Environment set up as per `docs/ENV_SETUP.md`.
    *   Poetry environment must be active (e.g., via `poetry shell` or by prefixing commands with `poetry run`).
*   **Data:**
    *   **Gold Syntactic Parses:** Your primary dataset (e.g., UD English EWT) should be placed in the `data/` directory. The files must be parseable by `src/torch_probe/utils/conllu_reader.py`, which expects columns for tokens, heads, UPOS tags, and XPOS tags.
    *   **Pre-computed Embeddings (for Stage 2):** HDF5 files generated by the Stage 1 scripts, corresponding to the CoNLL splits and target LLM layer.
*   **Configuration Files:** Familiarity with the Hydra configuration structure in the `configs/` directory. See `docs/ARCHITECTURE.md` for an overview.

## 3. Configuration Management with Hydra

All experiments are controlled via YAML files in `configs/` managed by Hydra.

*   **Main Configurations:**
    *   `configs/config_extract.yaml`: Base configuration for transformer embedding extraction.
    *   `configs/extraction/`: Directory for specific, one-off extraction configs.
    *   `configs/config.yaml`: Base configuration for probe training and evaluation.
*   **Configuration Groups:** Located in subdirectories under `configs/` (e.g., `configs/dataset/`, `configs/embeddings/`, `configs/probe/`, `configs/probe_rank`, `configs/logging`).
*   **Experiment Files:** Located in `configs/experiment/`. These compose configurations from different groups to define a complete probing experiment. Each experiment file **must** contain a `name` key at the top level (e.g., `name: hm_replication/bert-base-cased/dist/L7`), which is used to automatically name the output directory.
*   **Special Groups:** The `probe_rank` group is merged into the `probe` group at runtime using the `@` syntax (e.g., `- probe_rank@probe: "128"`), which is a clean way to manage this hyperparameter.

## 4. Stage 1: Running Embedding Extraction

### 4.1 For Hugging Face Transformer Models (e.g., BERT, Llama)

1.  **Prepare a Config:** Create a new YAML file in `configs/extraction/`, for example `configs/extraction/my_model_on_ud_ewt.yaml`, specifying the model, dataset paths, and output path. (See `configs/extraction/bert_base_cased_ud_ewt_all_layers.yaml` for an example).
2.  **Execute Script:** Run `extract_embeddings.py` by pointing it to your new config.
    ```bash
    poetry run python scripts/extract_embeddings.py --config-name=extraction/my_model_on_ud_ewt.yaml
    ```
3.  **Outputs:** HDF5 files will be saved in the directory specified by `output_hdf5.base_output_path`.

### 4.2 For Legacy ELMo

1.  **Convert CoNLL-U to Raw Text:** First, create a raw text file from your CoNLL-U data split.
    ```bash
    # Usage: python script.py <input_conllu> <output_txt>
    poetry run python scripts/convert_conllu_to_raw_generic.py \
      data/ud_english_ewt_official/en_ewt-ud-train.conllu \
      data_staging/ud_ewt_official_processed/raw_text/en_ewt-ud-train.txt
    ```
2.  **Generate ELMo Embeddings:** Run the generic shell script, providing the input raw text and desired output HDF5 path.
    ```bash
    # Usage: ./script.sh <input_txt> <output_hdf5>
    ./scripts/generate_elmo_embeddings_generic.sh \
      data_staging/ud_ewt_official_processed/raw_text/en_ewt-ud-train.txt \
      data_staging/ud_ewt_official_processed/elmo_hdf5_layers/en_ewt-ud-train.elmo-layers.hdf5
    ```

## 5. Stage 2: Running Probe Training & Evaluation

### 5.1. Key Evaluation Strategies

The evaluation pipeline has two important configurable strategies that affect metric calculation. These are controlled in your `evaluation/*.yaml` config files or via CLI override. An experiment selects its strategy by including either `/evaluation: default` or `/evaluation: eval_hm_metrics` in its `defaults` list.

*   **Punctuation Identification (`punctuation_strategy`):**
    *   **Goal:** To ignore punctuation tokens when calculating UUAS and Root Accuracy.
    *   **Options:**
        *   `"xpos"` (**The "H&M Way"**, set in `eval_hm_metrics.yaml`): Uses the original paper's method of filtering based on a hardcoded list of PTB-style XPOS tags (e.g., `.`, `,`, `` ` ``). Use this for replicating PTB results.
        *   `"upos"` (**The "Modern Way"**, set in `default.yaml`): Uses the language-agnostic Universal POS tags `PUNCT` and `SYM`. This is the recommended strategy for all new experiments.

*   **Spearman Correlation Length Filtering (`filter_by_non_punct_len`):**
    *   **Goal:** To filter sentences for the final DSpr/NSpr macro-average, excluding very short or very long sentences.
    *   **Options:**
        *   `false` (**The "H&M Way"**, set in `eval_hm_metrics.yaml`): Filters based on the sentence's original token count, including punctuation (the behavior of the original H&M code).
        *   `true` (**The "Modern Way"**, set in `default.yaml`): Filters based on the number of non-punctuation tokens in the sentence. This provides a more consistent measure of sentence complexity.

### 5.2. Learning Rate Scheduling & Checkpointing

The training script supports an H&M-style LR decay with optimizer reset (`training.lr_scheduler_with_reset`) and granular checkpointing options, configured in your `training/*.yaml` or experiment files.

### 5.3. Running a Single Experiment

**Command Structure:**
```bash
poetry run python scripts/train_probe.py experiment=<path/to/experiment_file>
```
*   **Example: Running the H&M replication for BERT Layer 7 on PTB:**
    ```bash
    poetry run python scripts/train_probe.py experiment=hm_replication/bert-base-cased/dist/L7
    ```
*   **Example: Running the ELMo Layer 1 Distance Probe Baseline on UD EWT:**
    ```bash
    poetry run python scripts/train_probe.py experiment=ud_ewt/elmo/dist/L1
    ```

### 5.4. Running Sweeps and Smoke Tests

*   **Sweeps with Hydra Multirun:** Use the `-m` or `--multirun` flag to sweep over parameters.
    ```bash
    # Example: Sweeping ELMo layers for a distance probe on UD EWT
    poetry run python scripts/train_probe.py -m \
      experiment=ud_ewt/elmo/dist/L0,ud_ewt/elmo/dist/L1,ud_ewt/elmo/dist/L2
    ```
*   **Smoke Testing:** To quickly verify that all configurations are valid without running a full experiment, a suite of test scripts is provided in `scripts/smoke_tests/`.
    *   **Run all smoke tests:**
        ```bash
        ./scripts/test_all_configs.sh
        ```
    *   **Run a specific suite of tests (e.g., for H&M Replication):**
        ```bash
        ./scripts/smoke_tests/02_test_hm_replication.sh
        ```

### 5.5. Speeding Up Runs for Debugging

To quickly debug a single experiment without waiting for the full training or evaluation to complete, you can override two special parameters on the command line.

*   `training.limit_train_batches`: Restricts the training loop to a specific number of batches per epoch.
*   `training.limit_eval_batches`: Restricts the validation loop to a specific number of batches.

**Example: Debugging the PTB replication by running only 5 training and 5 validation batches.**
```bash
poetry run python scripts/train_probe.py \
  experiment=hm_replication/bert-base-cased/dist/L7 \
  ++training.limit_train_batches=5 \
  ++training.limit_eval_batches=5
```

## 6. Interpreting Outputs from `train_probe.py`

Each run creates a unique output directory named after the experiment containing:
*   **`.hydra/`:** Resolved configuration files (`config.yaml`, `hydra.yaml`, `overrides.yaml`).
*   **`train_probe.log`:** Main log file from the script.
*   **`checkpoints/`:**
    *   `<probe_type>_probe_rank<X>_epoch<N>_metric<Y>.pt`: Saved based on `is_best`, `save_every_epoch_checkpoint`, or `save_checkpoint_every_n_epochs`.
    *   `<probe_type>_probe_rank<X>_best.pt`: Checkpoint with the best dev set performance on `training.early_stopping_metric`.
*   **`metrics_summary.json`:** Key final metrics (best dev score, epochs completed, test set scores).
*   **`dev_detailed_metrics_epochN.json` / `train_detailed_metrics_epochN.json`:** Per-epoch detailed metrics (per-sentence scores, etc.).
*   **`test_detailed_metrics_final.json`:** Detailed metrics for the test set evaluation.
*   **Plot images** if `logging.enable_plots=true`.

## 7. Weights & Biases Integration

*   Controlled by `logging.wandb.*` config settings.
*   If `logging.wandb.enable=true`, the script will log hyperparameters, metrics, and optionally model artifacts. Ensure `wandb login` has been performed.

## 8. Adding New Datasets or Models

1.  **New Dataset:** Prepare CoNLL-U files, place them in `data/`, and create a new YAML file in `configs/dataset/` defining the paths.
2.  **New Embeddings:** Run the appropriate Stage 1 script (Section 4) to generate HDF5 files. Create a new YAML file in `configs/embeddings/` pointing to these files.
3.  **New Experiment:** Create a YAML file in `configs/experiment/` that composes your new dataset and embedding configs, along with probe and training settings. **Crucially, this file must include a `name:` key that matches its path within `configs/experiment` (e.g., `name: my_group/my_model/my_probe`).**
4.  **Run:** Execute `scripts/train_probe.py` with your new experiment file.