# Build / Debug History (structural-probe-repl)

Last updated: 2025-07-16

This document tracks significant milestones, challenges, and resolutions encountered during the setup and development of the project environments and codebase.

| Date (2025) | Phase / Milestone / Task                  | Key Challenge(s) / Discovery                                                                  | Outcome / Fix / Decision                                                                                                                                                                 |
|-------------|-------------------------------------------|-----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Phase 0: Environment Setup & Legacy Probe Replication** |                                                                                               |                                                                                                                                                                              |
| May 02      | Native Env Setup (Poetry)                 | `poetry self add poetry-plugin-export` failed with Homebrew-installed Poetry.                 | Unlinkable brew files. Re-installed Poetry via `pipx` for user-space isolation; plugin installed cleanly.                                                                  |
| May 02      | Initial Legacy Env Plan (Assumed TF1)     | Project README for H&M code was not yet consulted.                                            | Initial plan assumed TensorFlow 1.x for legacy probe.                                                                                                                      |
| May 06      | Docker Build (TF1 Attempt)                | TF1.15 `amd64` binary (AVX) failed on Apple Silicon (M3) via Rosetta 2.                       | `Illegal instruction`. Confirmed AVX issue.                                                                                                                                |
| May 06      | H&M Code Review                           | GitHub README for `john-hewitt/structural-probes` reviewed.                                   | **Critical Discovery:** Original H&M code is PyTorch-based (~v1.0-1.3), not TensorFlow.                                                                                     |
| May 06      | Strategy Pivot (Legacy Probe)             | Need for TF1 environment eliminated.                                                          | Switched plan to target a PyTorch 1.x CPU environment (`linux/amd64`) for legacy code.                                                                                       |
| May 06      | Docker Base Image (Legacy PT)             | Difficulty finding specific `pytorch/pytorch:X.Y.Z-cpu-py37` tags.                            | Decided to use `python:3.7-slim-buster` as base and install PyTorch 1.x + dependencies via `pip`.                                                                          |
| May 06      | Docker Build (Legacy PT - Iteration 1)    | `pip install torch==1.1.0+cpu` failed.                                                        | `No matching distribution`. `pip` listed `1.3.0+cpu` as available.                                                                                                         |
| May 06      | Docker Build (Legacy PT - Iteration 2)    | Installed `torch==1.3.0+cpu`, `torchvision==0.4.1+cpu`, `numpy==1.19.5`, etc.                 | **Build Succeeded.** Core legacy dependencies installed.                                                                                                                   |
| May 06-07   | Legacy Container Runtime Debug            | Shebang typos, script arg passing errors, incorrect config paths, `PyYAML` API change, AllenNLP `TypeError` (`ArrayField`), Docker `WORKDIR`/`COPY` path issues, nested `.git` in `COPY`, Docker comment parsing. | Iteratively fixed scripts, Dockerfile, pinned `PyYAML==3.13`, `overrides==3.1.0`, `typing-extensions==3.7.4`. Used `.dockerignore`. Environment stabilized.               |
| May 07-23   | Legacy Example Data Handling              | H&M `download_example.sh` URLs dead. Initial self-prepared ELMo HDF5s caused `AssertionError` with original H&M `data.py` due to MWT counting. | Discovered `whykay-01` fork with original H&M sample data (CoNLLU, ELMo HDF5, BERT `.params`). Reverted `data.py` to original H&M version. Updated Dockerfile to `COPY` sourced data. |
| May 23      | **Phase 0a: Legacy Env & Examples**       | All H&M examples (ELMo training, BERT demo) now functional using data from `whykay-01` fork and original H&M `data.py`. | **COMPLETE.** Legacy probe pipeline verified. UUAS ~0.27 (ELMo dist), Spearman ~0.45 (ELMo dist), RootAcc ~0.08 (ELMo depth), BERT demo visualizations generated. |
| **Phase 1: Modern PyTorch Probe Re-implementation** |                                                                                               |                                                                                                                                                                              |
| May 23      | MS1.0: Basic Data Utilities               | -                                                                                             | Implemented `conllu_reader.py` (MWT-filtering) & `gold_labels.py` (depths/distances). Unit tests (own + independent) pass.                                                |
| May 24      | MS1.1: PyTorch Dataset & DataLoader     | Initial `pytest` `ModuleNotFoundError`. Mismatches in independent tests (HDF5 shape, head indexing, empty batch). | Fixed `pyproject.toml` & test imports. Corrected independent tests & `collate_fn` for empty batch. All `Dataset` tests pass.                                       |
| May 24      | MS1.2: Probe Models & Loss Fns          | `NameError` (Optional), `fixture 'self'` error in tests. Mismatch in independent loss test normalization. | Imported `Optional`. Fixed test fns. Corrected independent loss test to match H&M $L^2$ normalization. All model/loss tests pass.                               |
| May 24-26   | MS1.3 (Utils): Evaluate & Train Utils   | `NameError`s (`total_sentences`, `upos_full`), `IndentationError` in `evaluate.py`. `AttributeError` in `EarlyStopper` test. Optimizer LR check in checkpoint test. Spearman test logic refinement. | Fixed variable scoping & indentation. Corrected `EarlyStopper` property. Updated optimizer LR test assertion. Refined `calculate_spearmanr` to match H&M. All utility tests pass. |
| May 26      | MS1.3 (Main Script): `train_probe.py`   | Initial Hydra config composition error (`dataset@experiment.dataset`). `AssertionError` with `whykay-01` HDF5s & modern `conllu_reader`. Smoke test output path issues, checkpoint filename. | Resolved Hydra config by ensuring `experiment/*.yaml` pointed to self-generated HDF5s. Fixed smoke test output path logic & checkpoint naming. Implemented training script. |
| May 26      | **MS1.4: Full Validation (ELMo Sample)**| -                                                                                             | Successfully ran modern `train_probe.py` for distance & depth probes on self-generated ELMo sample using MPS. Achieved plausible metrics.        |
| May 26      | **Phase 1, MS1.3 (Main Script): `train_probe.py` Implementation**              | Initial Hydra config error (`dataset@experiment.dataset`). `AssertionError` with `whykay-01` HDF5s & modern `conllu_reader`. `UnboundLocalError` in training script. Output path issues & checkpoint naming in smoke test. | Resolved Hydra config. Switched modern probe to use self-generated ELMo HDF5s. Fixed `best_dev_metric_value_for_checkpointing` init. Corrected smoke test output path logic & checkpoint naming. Implemented H&M-style optimizer reset and LR decay (`LRSchedulerWithOptimizerReset`). |
| May 26      | **Phase 1, MS1.4: Full Validation on ELMo Sample (Modern Probe)**              | -                                                                                                                                                            | Successfully ran `train_probe.py` for both distance & depth probes on self-generated ELMo sample data using MPS. H&M LR decay/optimizer reset verified. Achieved plausible metrics. **Phase 1 COMPLETE.** |

| **Phase 2: PTB Data Processing & H&M Replication Baseline** |                                                                                               |                                                                                                                                                                              |
| May 27-28   | PTB to Stanford Deps (CoNLL-X)          | `ClassNotFoundException` for `EnglishGrammaticalStructure`. Initial CoreNLP command parsed PTB S-expressions as text. | Used H&M's script as guide for `java edu.stanford.nlp.trees.EnglishGrammaticalStructure` with correct flags (`-cp "*"`, `-basic`, `-keepPunct`, `-conllx`). Successfully generated PTB-SD CoNLL-X. |
| May 28      | Update `conllu_reader.py` & `dataset.py`  | Needed to parse CoNLL-X and extract/propagate XPOS tags for H&M punctuation alignment.        | Modified `read_conll_file` for XPOS. Updated `ProbeDataset` and `collate_fn`. Fixed `ProbeDataset.__del__`. Verified on new `.conllx`.                                |
| May 28      | Align `evaluate.py` with H&M Metrics    | Spearman averaging needed H&M's two-stage method. UUAS/RootAcc needed XPOS punctuation. Test updates. | Re-implemented Spearman (`calculate_spearmanr_hm_style`). UUAS/RootAcc now use H&M XPOS punct list. Updated unit tests for `evaluate.py`.                                 |
| May 28      | `extract_embeddings.py` Development     | `NameError`s (`ListConfig`, `HydraConfig`), Hydra interpolation key errors.                 | Added imports. Corrected Hydra config for `config_extract.yaml`. Script successfully extracts `bert-base-cased` (all layers) embeddings for full PTB-SD with word-alignment. |
| May 28      | Granular Checkpointing & Final Pipeline Polish | Implement flexible checkpointing. Ensure all tests pass with final pipeline. Test W&B E2E. | Added `save_every_epoch_checkpoint` & `save_checkpoint_every_n_epochs` logic to `train_probe.py` and configs. All tests pass. W&B logging fully functional.             |
| May 28      | **H&M Replication Run (BERT-b L7 Dist)**  | Ensuring all H&M hyperparameters, data paths, and metrics align for a full PTB run.          | Configured specific experiment. Ran full probe training for BERT-base L7 Distance. **Achieved UUAS ~80.75%, DSpr(HM) ~80.90% on test set, successfully replicating H&M Table 1.** |
| **(Current/Next)** | **Phase 3: Systematic Probing of Modern LLMs** | Planning next models (e.g., BERT-Large, ELMo baselines, then modern LLMs). Optimizing train-eval frequency. | Begin embedding extraction for next set of models.                                                              | **(NEW) Phase 2b: Pivot to Universal Dependencies** |                                                                                               |                                                                                                                                                                              |
| June 10-15  | UD Data Integration & ELMo Prep           | Needed to prepare full UD English EWT dataset. Refactored data prep scripts to be generic instead of sample-specific. | Acquired UD EWT CoNLL-U files. Used new generic scripts (`convert_conllu_to_raw_generic.py`, `generate_elmo_embeddings_generic.sh`) to create ELMo HDF5s for all splits. |
| June 16-17  | UD Baseline Runs (ELMo & BERT)            | Monitor metric for experiments switched to UUAS for more direct performance tracking. Long training times for full dataset. | Configured and ran Distance Probes on full UD EWT for ELMo (L0, L1, L2) and BERT-L7. Confirmed expected performance patterns, establishing strong baselines for UD. |
| **(Current/Next)** | **Phase 3: Systematic Probing of Modern LLMs on UD** | Planning next models for probing (e.g., Llama, Mistral) on UD EWT.                               | Begin embedding extraction for next set of models using `scripts/extract_embeddings.py`.                                                                               |

| **Phase 2c: Configuration & Testing Refactor** |                                                                                               |                                                                                                                                                                              |
|-------------|-------------------------------------------|-----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| July 16     | Hydra Config Standardization              | Initial test runs revealed Hydra errors (`InterpolationKeyError`, `Could not append...`).       | Discovered inconsistencies in experiment configs. **Standardized all `experiment/*.yaml` files to include a `name` key.** This resolved the `InterpolationKeyError` for `${experiment.name}`. |
| July 16     | Smoke Test Optimization                   | Smoke tests were too slow (~15-30 mins per run), as they evaluated on the full train/dev sets. | **Added `limit_train_batches` and `limit_eval_batches` to `train_probe.py` and configs.** This allows tests to run on a small subset of data, reducing test time to seconds. |
| July 16     | Smoke Test Script Refactor                | Single `test_all_configs.sh` was monolithic, making it hard to debug a specific experiment set. | **Replaced the single script with a modular suite in `scripts/smoke_tests/`.** Created separate scripts for H&M replication, baselines, etc., allowing for isolated and faster re-testing. |
| July 16     | Hydra Override Refinement                 | Test scripts failed due to `+` vs. no-prefix override conflicts between different training configs (`adam.yaml` vs `training_hm_replication.yaml`). | **Updated test scripts to use `++key=value` syntax** for command-line overrides. This robustly adds the key if it doesn't exist or overrides it if it does, handling all config variations correctly. |
| July 16     | **Milestone: Fast, Modular Testing**      | The experimental framework is now fully refactored, and all configurations pass a rapid smoke test. | The project has a stable and scalable configuration and testing setup, ready for the next phase of systematic probing.                             |
| July 17           | `extract_embeddings.py` Refactor                                                                                | Discovered architectural inconsistency where `extract_embeddings.py` used a separate, monolithic config file. This led to several Hydra errors (`Config group is mandatory`, `Key not in struct`) during testing. The alignment logic was also discovered to be local to the script and not reusable. | Refactored `extract_embeddings.py` to use its own main config (`config_extract.yaml`) and compose `dataset` and `model` groups from the command line. Moved alignment logic to a central utility `src/torch_probe/utils/alignment.py`. Created a master script to generate all canonical embeddings. The entire project now uses a consistent and robust configuration paradigm for both training and extraction. |

(This history will be appended as the project progresses.)